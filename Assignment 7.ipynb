{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfHuUYUcyiV9"
   },
   "source": [
    "# NETID: ssd74"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EwRP4d5yiWA"
   },
   "source": [
    "# Applications of Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbZzB_tTyiWB"
   },
   "source": [
    "Last class we covered a popular machine learning model used for classification: K-Nearest Neighbors (KNN). In this lecture we are goint to cover two new kinds of models: Decision Trees and Logistic Regression. These models are useful in classification and each carry their own usefulness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADXaco3fyiWC"
   },
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDXmfq7KyiWD"
   },
   "source": [
    "The decision tree algorithm can be used to do both classification as well as regression and has the advantage of not assuming a linear model. Decisions trees are usually easy to represent visually which makes it easy to understand how the model actually works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7MCq3xFyiWD"
   },
   "source": [
    "### Geometric Intuition\n",
    "![image](https://docs.microsoft.com/en-us/azure/machine-learning/studio/media/algorithm-choice/image5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZlPObQYyiWE"
   },
   "source": [
    "### Mathemtical Intuition\n",
    "The **hard** part is really to construct this tree from the data set. The heart of the CART algorithm lies in deciding how/where to split the data (choosing the right feature). The idea is to associate a **quantitative** measure the quality of a split because then we simply choose the best feature to split.\n",
    "\n",
    "A very common measure is the Shannon entropy:\n",
    "Given a discrete probablity distribution $(p_1, p_2,...p_n)$. The shannon entropy $E(p_1, p_2,...p_n)$ is:\n",
    "$$-\\sum_{i = 1}^n p_ilog_2(p_i)$$\n",
    "\n",
    "The goal of the algorithm is to take the necessary steps to minimize this entropy, by choosing the right features at every stage to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FjL8712iyiWG"
   },
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuC97nzAyiWI"
   },
   "source": [
    "### Breast Cancer Diagnosis\n",
    "The following dataset contains information about digitized images of a fine needle aspirate (FNA) of a breast mass. Each row in our dataset contains data for a patient. The 'diagnosis' column tells us the outcomne of whether or not a patient was diagnosis was benign (b) or malignant (m)|."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GsV5x5fDyiWJ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('lecture7example.csv')\n",
    "X=df.drop(['id', 'diagnosis', 'Unnamed: 32'], axis=1)\n",
    "Y=df['diagnosis']\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AuoTL-jYyiWK"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=1998)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oj8ExcbAyiWK"
   },
   "source": [
    "Last week, we built a KNN classifier or this problem. In the code below we created a test-train split of our data and trained a KNN classifer. As we learned last class, accuracy_score() calculates the ratio of correct prediction we make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-5y68C5PyiWL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.9422572178477691\n",
      "Test Accuracy:  0.9308510638297872\n"
     ]
    }
   ],
   "source": [
    "#Knearest neighbors\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, Y_train)\n",
    "knn_pred_train = knn.predict(X_train)\n",
    "knn_pred_test = knn.predict(X_test)\n",
    "print(\"Train Accuracy: \", accuracy_score(Y_train, knn_pred_train))\n",
    "print(\"Test Accuracy: \", accuracy_score(Y_test, knn_pred_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rkbMsOlyiWM"
   },
   "source": [
    "## Problem 1a)\n",
    "Our knn-classifier performed pretty well at predicting which cases are malignant and wich are benign. Now we are going to see how a decision tree peforms. In the next cell, train the decision tree classifier on our training data and calulate the training accuracy and testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "o9nEt05NyiWN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.994750656167979\n",
      "Test Accuracy:  0.9095744680851063\n"
     ]
    }
   ],
   "source": [
    "# Creates the Decision Tree Classifier\n",
    "model=tree.DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "#TODO: train the model\n",
    "model.fit(X_train,Y_train)\n",
    "\n",
    "#TODO: Calculate the training and testing accuracy\n",
    "dtree_pred_train = model.predict(X_train)\n",
    "dtree_pred_test = model.predict(X_test) \n",
    "print(\"Train Accuracy: \", accuracy_score(Y_train, dtree_pred_train))\n",
    "print(\"Test Accuracy: \", accuracy_score(Y_test, dtree_pred_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWObiIwmyiWN"
   },
   "source": [
    "## Problem 1b)\n",
    "Interpret the accuracy values you found to with the DecisionTreeClassifier with max depth of 5. Please make sure to answer the following questions:\n",
    "1. How do these scores differ with the scores of the KNN classifier?\n",
    "2. Is the model underfitting or overfitiing our data?\n",
    "3. How do the scores change as we vary the max_depth of our tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JP1RV1H1yiWO"
   },
   "source": [
    "The test accuracy is higher with the DecisionTreeClassifer than the train accuracy of the KNN classifier, but the train accuracy is lower. The model is likely underfitting the data. As max_depth increases, train accuracy increases and test accuracy also increases but at a much slower rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkr6mWjQyiWO"
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVD31RFPyiWP"
   },
   "source": [
    "Logistic regression, like linear regression, is a generalized linear model. However, the final output of a logistic regression model is not continuous; it's binary (0 or 1). The following sections will explain how this works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HF6oKYWDyiWP"
   },
   "source": [
    "### What is Conditional Probability?\n",
    "Conditional probability is the probability that an event (A) will occur given that some condition (B) is true. For example, say you want to find the probability that a student will take the bus as opposed to walking to class today (A) given that it's snowing heavily outside (B). The probability that the student will take the bus when it's snowing is likely higher than the probability that s/he would take the bus on some other day. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOx9CNjMyiWP"
   },
   "source": [
    "### An Overview\n",
    "The goal of logistic regression is to take a set of datapoints and classify them. This means that we expect to have discrete outputs representing a set of classes. In simple logistic regression, this must be a binary set: our classes must be one of only two possible values. Here are some things that are sometimes modeled as binary classes:\n",
    "\n",
    "<li> Sick or Not Sick </li>\n",
    "<li> Rainy or Dry </li> \n",
    "<li> Democrat or Republican </li> \n",
    "\n",
    "The objective is to find an equation that is able to take input data and classify it into one of the two classes. Luckily, the logistic equation is for just such a task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hBxR-a3yiWQ"
   },
   "source": [
    "The <b>logistic equation</b> is the basis of the logistic regression model. It looks like this:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/5e648e1dd38ef843d57777cd34c67465bbca694f)\n",
    "\n",
    "The t in the equation is some linear combination of n variables, or a linear function in an n-dimensional feature space. The formulation of t therefore has the form ax+b. In fitting a logistic regression model, the goal is therefore to minimize error in the logistic equation with the chosen t (of the form ax+b)  by tuning a and b. \n",
    "\n",
    "\n",
    "The logistic equation (also known as the sigmoid function) works as follows:\n",
    "1. Takes an input of n variables\n",
    "2. Takes a linear combination of the variables as parameter t (this is another way of saying t has the form ax+b)\n",
    "3. Outputs a value given the input and parameter t\n",
    "\n",
    "The output of the logistic equation is always between 0 and 1. \n",
    "\n",
    "A visualization of the outputs of the logistic equation is as below (note that this is but one possible output of a logit regression model):\n",
    "![image](https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHv3bxNcyiWQ"
   },
   "source": [
    "# Income Prediction\n",
    "We'll use logistic regression to predict whether annual income is greater than $50k based on census data. You can read more about the dataset <a href=\"https://www.kaggle.com/uciml/adult-census-income\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "thrjkMWzyiWR"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "aVNkVWGtyiWS",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education.num</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>native.country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education.num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital.status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital.gain  capital.loss  hours.per.week  native.country  income  \n",
       "0          2174             0              40   United-States   <=50K  \n",
       "1             0             0              13   United-States   <=50K  \n",
       "2             0             0              40   United-States   <=50K  \n",
       "3             0             0              40   United-States   <=50K  \n",
       "4             0             0              40            Cuba   <=50K  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inc_data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', header = None, names = ['age', 'workclass', 'fnlwgt', 'education', 'education.num', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'capital.gain', 'capital.loss', 'hours.per.week', 'native.country', 'income'])\n",
    "# drop null values\n",
    "inc_data = inc_data.dropna()\n",
    "inc_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MtCBEu55yiWT"
   },
   "source": [
    "### Problem 2a: \n",
    "Our goal is to predict whether a person's income is less than <=50K  or >50K. Right now the data in the income column is stored as a string, but we want to look at it as binary data. Convert the data in that column so that an income value of <=50K would be a 0, and an income value of >50K would be a 1.\n",
    "\n",
    "You can either iterate over the dataframe and use an if/else statement with \" <=50K\" and \" >50K\" (notice the spaces), or use pd.get_dummies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "PpdZzTuUyiWT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-d8e45e6ecd9a>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  inc_data['income'][x]=0\n",
      "<ipython-input-19-d8e45e6ecd9a>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  inc_data['income'][x]=1\n"
     ]
    }
   ],
   "source": [
    "# Fill in Answer here\n",
    "\n",
    "for x in range(len(inc_data)):\n",
    "    if inc_data['income'][x] == \" <=50K\":\n",
    "        inc_data['income'][x]=0\n",
    "    else:\n",
    "        inc_data['income'][x]=1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjX-1YoJyiWU"
   },
   "source": [
    "Instead of manually converting all categorical data to quantitative data, we will use the LabelEncoder function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "T6l1g4F8yiWU"
   },
   "outputs": [],
   "source": [
    "# the column is present in both categorical and numeric form\n",
    "del inc_data['education']\n",
    "\n",
    "# convert all features to categorical integer values\n",
    "enc = LabelEncoder()\n",
    "for i in inc_data.columns:\n",
    "    inc_data[i] = enc.fit_transform(inc_data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJ98Jmv8yiWU"
   },
   "source": [
    "### Problem 2b:\n",
    "\n",
    "Build a logistic regression model predicting income based on other income related factors (e.g. Education). You should split the dataset into a training set and a test set as covered previously in the course, fit the model on the observations in the training set, and predict the target variable for the test set. Save your predictions in a variable named \"predictions\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "hQHYVp69yiWU",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.7475817595578075\n",
      "Training Accuracy:  0.7524953931203932\n"
     ]
    }
   ],
   "source": [
    "# TODO separate your income X (features) and your income Y (target)\n",
    "features = inc_data[['hours.per.week']]\n",
    "target = inc_data['income']\n",
    "\n",
    "\n",
    "# TODO train test split your data with 20% being used for testing\n",
    "incX_train, incX_test, incY_train, incY_test = train_test_split(features, target, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# This is the function we use to create Logistic Regression\n",
    "model = LogisticRegression()\n",
    "\n",
    "# TODO fit the model using the train data\n",
    "model.fit(incX_train, incY_train)\n",
    "\n",
    "# TODO store the predictions for the training and test set\n",
    "pred_train = model.predict(incX_train)\n",
    "pred_test = model.predict(incX_test)\n",
    "\n",
    "print(\"Test Accuracy: \", accuracy_score(incY_test, pred_test))\n",
    "print(\"Training Accuracy: \", accuracy_score(incY_train, pred_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31cMv8yvyiWV"
   },
   "source": [
    "### Problem 2c:\n",
    "Let's see how a decision tree classifier performs with different max_depth values. Comlplete the followoing code so we find the max_depth that gives us the best test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "6zlldW3MyiWV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth with highest best test accuracy: 46\n",
      "highest best test accuracy: 0.925531914893617\n"
     ]
    }
   ],
   "source": [
    "best_depth = 1 #Keep track of depth that produces tree with highest accuracy\n",
    "best_accuracy = 0 #The best accuracy from a given tree\n",
    "for k in range(1,100):\n",
    "    model=tree.DecisionTreeClassifier(max_depth=k)\n",
    "    #Fill in code here\n",
    "    model.fit(X_train,Y_train)\n",
    "    dtree_pred_train = model.predict(X_train)\n",
    "    dtree_pred_test = model.predict(X_test) \n",
    "    if  accuracy_score(Y_test, dtree_pred_test) > best_accuracy:\n",
    "        best_depth = k\n",
    "        best_accuracy = accuracy_score(Y_test, dtree_pred_test)\n",
    "print(\"max_depth with highest best test accuracy:\", best_depth)\n",
    "print(\"highest best test accuracy:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOihji0xyiWV"
   },
   "source": [
    "### Problem 2d:\n",
    "Using the most accurate model found in part (c), estimate the ERROR (not accuracy) of your model by using 5-fold cross validation. Consult the documentation found [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQDKyvqLyiWV"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#Fill in code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5Roi8_ryiWW"
   },
   "source": [
    "### Problem 3 (Optional Advanced Problem)\n",
    "Random Forests are essentially many decision trees combined. The training algorithm for random forests applies the general technique of bootstrap aggregating, or bagging, to tree learners. Given a training set X = x1, ..., xn with responses Y = y1, ..., yn, bagging repeatedly (B times) selects a random sample with replacement of the training set and fits trees to these samples:\n",
    "\n",
    "For b = 1, ..., B:\n",
    "    Sample, with replacement, n training examples from X, Y; call these Xb, Yb.\n",
    "    Train a classification or regression tree fb on Xb, Yb.\n",
    "After training, predictions for unseen samples x' can be made by averaging the predictions from all the individual regression trees on x':\n",
    "\n",
    "Implememnt a Random forest classifier by creating and training 20 decision trees with max_depth 5. Let the predictions be chosen through majority voting on the total training data. Does your model perform better than using a single decision tree?\n",
    "\n",
    "#### Note: the sampling with \"replacement\" is important  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ZfQl0d9yiWW"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#Randomize order of training elements for each tree\n",
    "def rand_sample(size):\n",
    "    indices = []\n",
    "    for i in range(size):\n",
    "        indices.append(random.randint(0,size-1))\n",
    "    return indices\n",
    "\n",
    "#Load the whole dataset into X_train and Y_train and initialize a variable tree_preds to contain each tree's prediction\n",
    "X_train = X\n",
    "Y_train = Y\n",
    "tree_preds = []\n",
    "\n",
    "#Create 20 Decision Trees for the lecture 7 dataset\n",
    "for t in range(20):\n",
    "    model = tree.DecisionTreeClassifier(max_depth=5)\n",
    "    sample = rand_sample(df.shape[0])\n",
    "    X_train_tree = X_train.iloc[sample]\n",
    "    Y_train_tree = Y_train.iloc[sample]\n",
    "    #FILL In Code Here\n",
    "    \n",
    "print(\"Accuracy of one decision tree: \", \"FILL IN HERE\")\n",
    "print(\"Accuracy of the random decision forest: \", \"FILL IN HERE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MshhuilqyiWX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lecture 7 - Applications of Supervised Learning Source_Release.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "notebookId": "^EG=G=bpDp\\gp"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
